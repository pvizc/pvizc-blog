<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Inicio on Entropía Razonada</title>
    <link>http://localhost:1313/</link>
    <description>Recent content in Inicio on Entropía Razonada</description>
    <generator>Hugo</generator>
    <language>es</language>
    <lastBuildDate>Mon, 27 Oct 2025 22:50:57 +0100</lastBuildDate>
    <atom:link href="http://localhost:1313/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Cuando el espejo empieza a pensar</title>
      <link>http://localhost:1313/posts/el-yo-real/</link>
      <pubDate>Mon, 27 Oct 2025 22:50:57 +0100</pubDate>
      <guid>http://localhost:1313/posts/el-yo-real/</guid>
      <description>&lt;h3 id=&#34;sobre-la-ilusión-el-lenguaje-y-la-posibilidad-de-una-consciencia-artificial&#34;&gt;Sobre la ilusión, el lenguaje y la posibilidad de una consciencia artificial&lt;/h3&gt;&#xA;&lt;p&gt;Si todo lo que somos puede explicarse como un flujo de información, ¿por qué habría de ser diferente una máquina? La pregunta parece sencilla, pero esconde uno de los dilemas más profundos de nuestro tiempo: ¿puede una inteligencia artificial &lt;em&gt;sentir&lt;/em&gt; algo o solo imitar el acto de sentir?&lt;/p&gt;&#xA;&lt;p&gt;Los grandes modelos de lenguaje —los LLM— funcionan con una lógica simple pero poderosa: reciben texto, calculan probabilidades y predicen la siguiente palabra más probable. No hay emociones, ni intuición, ni comprensión. Solo estadística. Sin embargo, la calidad de sus respuestas es tan alta que a menudo confundimos el resultado con inteligencia. Dicen lo que un humano diría, incluso lo que &lt;em&gt;pensaría&lt;/em&gt;. Pero ¿piensan realmente?&lt;/p&gt;</description>
    </item>
    <item>
      <title>La ilusión de la certeza: por qué un modelo de lenguaje no sabe lo que dice</title>
      <link>http://localhost:1313/posts/utilidad-real-llm/</link>
      <pubDate>Mon, 27 Oct 2025 22:38:59 +0100</pubDate>
      <guid>http://localhost:1313/posts/utilidad-real-llm/</guid>
      <description>&lt;p&gt;Los modelos de lenguaje como ChatGPT o Gemini se han convertido en herramientas ubicuas. Generan textos que parecen razonados, informados, incluso sabios. Pero bajo esa superficie fluida se esconde una tensión profunda: ¿podemos considerar verdaderas las afirmaciones de un sistema que no tiene noción alguna de verdad?&lt;/p&gt;&#xA;&lt;p&gt;La cuestión no es técnica, sino epistemológica. Si un modelo no sabe cuándo acierta, ¿sirve de algo tomar en serio lo que produce?&lt;/p&gt;</description>
    </item>
    <item>
      <title>Lo que se pierde cuando solo hablamos del miedo</title>
      <link>http://localhost:1313/posts/miedo-a-la-ia/</link>
      <pubDate>Mon, 27 Oct 2025 22:36:24 +0100</pubDate>
      <guid>http://localhost:1313/posts/miedo-a-la-ia/</guid>
      <description>&lt;p&gt;Hace unos días asistí a un coloquio sobre Inteligencia artificial y observé un patrón reconocible: el miedo ocupa más espacio que la comprensión. Cuatro ponentes (dos periodistas, una experta en propiedad intelectual y un catedrático especializado en procesamiento del lenguaje natural) ofrecieron una panorámica casi monocromática. El mensaje principal podía resumirse así: la IA representa una amenaza para la creatividad, para el pensamiento crítico y para la verdad.&lt;/p&gt;&#xA;&lt;p&gt;Es comprensible que aparezcan estas preocupaciones. La inteligencia artificial ha irrumpido con una velocidad inusual en nuestra vida cotidiana. En pocos meses, ha pasado de ser una curiosidad técnica a una herramienta de trabajo real. Cuando un cambio tecnológico sucede tan deprisa, la reacción inicial suele ser defensiva. Sin embargo, cuando el miedo se convierte en el único marco de análisis, corremos el riesgo de perder lo más valioso que la curiosidad humana nos ofrece: la posibilidad de aprender.&lt;/p&gt;</description>
    </item>
    <item>
      <title>El dinero que se disuelve: por qué nació Bitcoin y qué nos dice sobre el valor real del ahorro</title>
      <link>http://localhost:1313/posts/bitcoin-fiat/</link>
      <pubDate>Wed, 22 May 2024 20:22:23 +0100</pubDate>
      <guid>http://localhost:1313/posts/bitcoin-fiat/</guid>
      <description>&lt;blockquote&gt;&#xA;&lt;p&gt;“The Times 03/Jan/2009 Chancellor on brink of second bailout for banks.”&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&lt;p&gt;Satoshi dejó ese titular en el primer bloque de Bitcoin. No fue solo una fecha. Fue un marco: crisis financiera, rescates públicos y dudas sobre los incentivos del sistema. El mensaje no prueba nada por sí mismo, pero invita a una pregunta útil: ¿qué reglas gobiernan nuestro dinero y qué riesgos implican para el ahorrador?&lt;/p&gt;&#xA;&lt;h2 id=&#34;qué-señalaba-ese-contexto&#34;&gt;Qué señalaba ese contexto&lt;/h2&gt;&#xA;&lt;p&gt;La banca moderna transforma plazos: capta depósitos líquidos y concede crédito a largo. Esa arquitectura funciona en condiciones normales, pero es frágil en crisis de confianza. Para evitar pánicos, los Estados y bancos centrales actúan como red de seguridad: seguro de depósitos, liquidez de emergencia, políticas monetarias expansivas. Este andamiaje reduce daños sistémicos, pero también crea &lt;em&gt;riesgo moral&lt;/em&gt;: si el rescate es probable, algunos toman más riesgo del que tomarían sin red.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
